@inproceedings{alam2016multiagent,
 abstract = {Fog computing, which performs on network edges, is a front-end distributed computing archetype of centralized cloud computing. Mobile Fog is a special purpose computing prototype, which leverages the mobile computing to deliver seamless and latency-aware mobile services. Offloading computation in mobile Fog is challenging because of the spatiotemporal resource requirements of heterogeneous mobile devices. In this paper, we propose reinforcement learning based code offloading mechanism to ensure low-latency service delivery towards mobile service consumers. We use the distributed reinforcement learning algorithm to offload basic blocks in a decentralized fashion to deploy mobile codes on geographically distributed mobile Fogs. We simulate the proposed prototype using OMNeT++ considering fluctuated resources of mobile Fog and varied service demands of mobile users. The proposed method significantly reduces the execution time and latency of accessing mobile services while ensuring lower energy consumption of mobile devices.},
 author = {Alam, M. G. R. and Tun, Y. K. and Hong, C. S.},
 booktitle = {2016 {{International Conference}} on {{Information Networking}} ({{ICOIN}})},
 doi = {10.1109/ICOIN.2016.7427078},
 month = {jan},
 pages = {285-290},
 title = {Multi-Agent and Reinforcement Learning Based Code Offloading in Mobile Fog},
 year = {2016}
}

@article{Chen2016EfficientMultiUserComputation,
 abstract = {Mobile-edge cloud computing is a new paradigm to provide cloud computing capabilities at the edge of pervasive radio access networks in close proximity to mobile users. In this paper, we first study the multi-user computation offloading problem for mobile-edge cloud computing in a multi-channel wireless interference environment. We show that it is NP-hard to compute a centralized optimal solution, and hence adopt a game theoretic approach for achieving efficient computation offloading in a distributed manner. We formulate the distributed computation offloading decision making problem among mobile device users as a multi-user computation offloading game. We analyze the structural property of the game and show that the game admits a Nash equilibrium and possesses the finite improvement property. We then design a distributed computation offloading algorithm that can achieve a Nash equilibrium, derive the upper bound of the convergence time, and quantify its efficiency ratio over the centralized optimal solutions in terms of two important performance metrics. We further extend our study to the scenario of multi-user computation offloading in the multi-channel wireless contention environment. Numerical results corroborate that the proposed algorithm can achieve superior computation offloading performance and scale well as the user size increases.},
 author = {Chen, X. and Jiao, L. and Li, W. and Fu, X.},
 doi = {10.1109/TNET.2015.2487344},
 journal = {IEEE/ACM Transactions on Networking},
 month = {oct},
 number = {5},
 pages = {2795-2808},
 title = {Efficient {{Multi}}-{{User Computation Offloading}} for {{Mobile}}-{{Edge Cloud Computing}}},
 volume = {24},
 year = {2016}
}

@article{chen2018computation,
 abstract = {The (ultra-)dense deployment of small-cell base stations (SBSs) endowed with cloud-like computing functionalities paves the way for pervasive mobile edge computing, enabling ultra-low latency and location-awareness for a variety of emerging mobile applications and the Internet of Things. To handle spatially uneven computation workloads in the network, cooperation among SBSs via workload peer offloading is essential to avoid large computation latency at overloaded SBSs and provide high quality of service to end users. However, performing effective peer offloading faces many unique challenges due to limited energy resources committed by self-interested SBS owners, uncertainties in the system dynamics, and co-provisioning of radio access and computing services. This paper develops a novel online SBS peer offloading framework, called online peer offloading (OPEN), by leveraging the Lyapunov technique, in order to maximize the long-term system performance while keeping the energy consumption of SBSs below individual long-term constraints. OPEN works online without requiring information about future system dynamics, yet provides provably near-optimal performance compared with the oracle solution that has the complete future information. In addition, this paper formulates a peer offloading game among SBSs and analyzes its equilibrium and efficiency loss in terms of the price of anarchy to thoroughly understand SBSs' strategic behaviors, thereby enabling decentralized and autonomous peer offloading decision making. Extensive simulations are carried out and show that peer offloading among SBSs dramatically improves the edge computing performance.},
 author = {Chen, L. and Zhou, S. and Xu, J.},
 doi = {10.1109/TNET.2018.2841758},
 issn = {1063-6692},
 journal = {IEEE/ACM Transactions on Networking},
 month = {aug},
 number = {4},
 pages = {1619-1632},
 title = {Computation {{Peer Offloading}} for {{Energy}}-{{Constrained Mobile Edge Computing}} in {{Small}}-{{Cell Networks}}},
 volume = {26},
 year = {2018}
}

@inproceedings{cuervo2010maui,
 abstract = {This paper presents MAUI, a system that enables fine-grained energy-aware offload of mobile code to the infrastructure. Previous approaches to these problems either relied heavily on programmer support to partition an application, or they were coarse-grained requiring full process (or full VM) migration. MAUI uses the benefits of a managed code environment to offer the best of both worlds: it supports fine-grained code offload to maximize energy savings with minimal burden on the programmer. MAUI decides at run-time which methods should be remotely executed, driven by an optimization engine that achieves the best energy savings possible under the mobile device's current connectivity constrains. In our evaluation, we show that MAUI enables: 1) a resource-intensive face recognition application that consumes an order of magnitude less energy, 2) a latency-sensitive arcade game application that doubles its refresh rate, and 3) a voice-based language translation application that bypasses the limitations of the smartphone environment by executing unsupported components remotely.},
 address = {{New York, NY, USA}},
 author = {Cuervo, Eduardo and Balasubramanian, Aruna and Cho, Dae-ki and Wolman, Alec and Saroiu, Stefan and Chandra, Ranveer and Bahl, Paramvir},
 booktitle = {Proceedings of the 8th {{International Conference}} on {{Mobile Systems}}, {{Applications}}, and {{Services}}},
 doi = {10.1145/1814433.1814441},
 isbn = {978-1-60558-985-5},
 pages = {49--62},
 publisher = {{ACM}},
 series = {{{MobiSys}} '10},
 shorttitle = {{{MAUI}}},
 title = {{{MAUI}}: {{Making Smartphones Last Longer}} with {{Code Offload}}},
 year = {2010}
}

@inproceedings{geng2018energyefficient,
 abstract = {Modern mobile devices are equipped with multicore-based processors, which introduce new challenges on computation offloading. With the big.LITTLE architecture, instead of only deciding locally or remotely running a task in the traditional architecture, we have to consider how to exploit the new architecture to minimize energy while satisfying application completion time constraints. In this paper, we address the problem of energy-efficient computation offloading on multicore-based mobile devices running multiple applications. We first formalize the problem as a mixed-integer nonlinear programming problem that is NP-hard, and then propose a novel heuristic algorithm to jointly solve the offloading decision and task scheduling problems. The basic idea is to prioritize tasks from different applications to make sure that both application time constraints and task-dependency requirements are satisfied. To find a better schedule while reducing the schedule searching overhead, we propose a critical path based solution which recursively checks the tasks and moves tasks to the right CPU cores to save energy. Simulation and experimental results show that our offloading algorithm can significantly reduce the energy consumption of mobile devices while satisfying the application completion time constraints.},
 author = {Geng, Y. and Yang, Y. and Cao, G.},
 booktitle = {Proceedings of {{IEEE Conference}} on {{Computer Communications}} ({{INFOCOM}} 2018)},
 doi = {10.1109/INFOCOM.2018.8485875},
 month = {apr},
 pages = {46-54},
 title = {Energy-{{Efficient Computation Offloading}} for {{Multicore}}-{{Based Mobile Devices}}},
 year = {2018}
}

@article{guo2019energyefficient,
 abstract = {Mobile cloud computing (MCC) as an emerging and prospective computing paradigm, can significantly enhance computation capability and save energy for smart mobile devices (SMDs) by offloading computation-intensive tasks from resource-constrained SMDs onto resource-rich cloud. However, how to achieve energy-efficient computation offloading under hard constraint for application completion time remains a challenge. To address such a challenge, in this paper, we provide an energy-efficient dynamic offloading and resource scheduling (eDors) policy to reduce energy consumption and shorten application completion time. We first formulate the eDors problem into an energy-efficiency cost (EEC) minimization problem while satisfying task-dependency requirement and completion time deadline constraint. We then propose a distributed eDors algorithm consisting of three subalgorithms of computation offloading selection, clock frequency control, and transmission power allocation. Next, we show that computation offloading selection depends on not only the computing workload of a task, but also the maximum completion time of its immediate predecessors and the clock frequency and transmission power of the mobile device. Finally, we provide experimental results in a real testbed and demonstrate that the eDors algorithm can effectively reduce EEC by optimally adjusting CPU clock frequency of SMDs in local computing, and adapting the transmission power for wireless channel conditions in cloud computing.},
 author = {Guo, S. and Liu, J. and Yang, Y. and Xiao, B. and Li, Z.},
 doi = {10.1109/TMC.2018.2831230},
 issn = {1536-1233},
 journal = {IEEE Transactions on Mobile Computing},
 month = {feb},
 number = {2},
 pages = {319-333},
 title = {Energy-{{Efficient Dynamic Computation Offloading}} and {{Cooperative Task Scheduling}} in {{Mobile Cloud Computing}}},
 volume = {18},
 year = {2019}
}

@inproceedings{kosta2012thinkair,
 abstract = {Smartphones have exploded in popularity in recent years, becoming ever more sophisticated and capable. As a result, developers worldwide are building increasingly complex applications that require ever increasing amounts of computational power and energy. In this paper we propose ThinkAir, a framework that makes it simple for developers to migrate their smartphone applications to the cloud. ThinkAir exploits the concept of smartphone virtualization in the cloud and provides method-level computation offloading. Advancing on previous work, it focuses on the elasticity and scalability of the cloud and enhances the power of mobile cloud computing by parallelizing method execution using multiple virtual machine (VM) images. We implement ThinkAir and evaluate it with a range of benchmarks starting from simple micro-benchmarks to more complex applications. First, we show that the execution time and energy consumption decrease two orders of magnitude for a N-queens puzzle application and one order of magnitude for a face detection and a virus scan application. We then show that a parallelizable application can invoke multiple VMs to execute in the cloud in a seamless and on-demand manner such as to achieve greater reduction on execution time and energy consumption. We finally use a memory-hungry image combiner tool to demonstrate that applications can dynamically request VMs with more computational power in order to meet their computational requirements.},
 author = {Kosta, S. and Aucinas, A. and {Pan Hui} and Mortier, R. and {Xinwen Zhang}},
 booktitle = {Proceedings of {{IEEE Conference}} on {{Computer Communications}} ({{INFOCOM}} 2012)},
 doi = {10.1109/INFCOM.2012.6195845},
 month = {mar},
 pages = {945-953},
 shorttitle = {{{ThinkAir}}},
 title = {{{ThinkAir}}: {{Dynamic}} Resource Allocation and Parallel Execution in the Cloud for Mobile Code Offloading},
 year = {2012}
}

@article{meskar2017energy,
 abstract = {This paper considers a set of mobile users that employ cloud-based computation offloading. In order to execute jobs in the cloud, the user uploads must occur over a base station channel that is shared by all of the uploading users. Since the job completion times are subject to hard deadline constraints, this restricts the feasible set of jobs that can be processed. The system is modelled as a competitive game in which each user is interested in minimizing its own energy consumption. The game is subject to the real-time constraints imposed by the job execution deadlines, user specific channel bit rates, and the competition over the shared communication channel. The paper shows that for a wide range of parameters, a game where each user independently sets its offloading decisions always has a pure Nash equilibrium, and a Gauss-Seidel-like method for determining this equilibrium is introduced. Results are presented that illustrate that the system always converges to a Nash equilibrium using the Gauss-Seidel method. Data is also presented that show the number of iterations required, and the quality of the solutions. We find that the solutions perform well compared to a lower bound on total energy performance.},
 author = {Meskar, E. and Todd, T. D. and Zhao, D. and Karakostas, G.},
 doi = {10.1109/TMC.2016.2538227},
 issn = {1536-1233},
 journal = {IEEE Transactions on Mobile Computing},
 month = {jan},
 number = {1},
 pages = {87-96},
 title = {Energy {{Aware Offloading}} for {{Competing Users}} on a {{Shared Communication Channel}}},
 volume = {16},
 year = {2017}
}

